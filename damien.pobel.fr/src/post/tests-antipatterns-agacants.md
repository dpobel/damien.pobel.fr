---
title: "Tests¬†: mon top 8 des anti-patrons les plus aga√ßants"
tags: bonnes pratiques, unit test, qualit√©, code, behaviour driven development, tdd, php, javascript, ing√©nierie logicielle
lang: fr
published: 2021-04-08
photos:
    - images/angry-cat.jpg
---

Il y a un peu plus trois ans, je publiais [Au fait, c'est quoi un bon test
unitaire, d'int√©gration ou
fonctionnel¬†?](/post/bon-test-unitaire-integration-fonctionnel), un billet o√π je
d√©crivais quelques caract√©ristiques d'une bonne suite de tests logiciel. Ce
billet reste d'actualit√© mais depuis, je me suis frott√© √† plusieurs
environnements accumulant parfois plusieurs _antipatterns_, certains
particuli√®rement p√©nibles. Alors cette fois ci, je prends le sujet dans l'autre
sens, voici une s√©lection des anti-patrons li√©s aux tests que j'ai pu crois√©s
plus ou moins r√©cemment.


<figure class="object-center bordered">
    <img loading="lazy" src="/images/660x/angry-cat.jpg" alt="Un chat donnant l'impression d'√™tre agac√©">
    <footer>
    Photo par <a
    href="https://pixabay.com/users/skitterphoto-324082/">Skitterphoto</a>
    </footer>
</figure>

## Les tests pas lanc√©s automatiquement

Une suite de tests qui n'est pas lanc√©e automatiquement ne sert (quasiment) √†
rien. Bien s√ªr, si on se lance dans un effort pour √©crire des tests (en
particulier des tests _end to end_ ou fonctionnels), cet √©tat peut √™tre
_transitoire_ le temps d'investir suffisamment de temps pour automatiser
l'ensemble mais clairement il faut essayer d'en sortir le plus rapidement
possible sinon arrivera invariablement le moment o√π un¬∑e d√©veloppeur¬∑se cassera
quelque chose sans s'en rendre compte et g√©n√©ralement, il faut pas longtemps üòâ

Dans cette cat√©gorie, il arrive aussi qu'une partie des tests ne soient pas
lancer automatiquement, par exemple lors de l'ajout de code dans un endroit pour
lequel le _test runner_ n'est pas configur√©. C'est assez vicieux et pas
forc√©ment √©vident √† d√©tecter; on peut √©ventuellement
surveiller l'√©volution [du taux de couverture et/ou jeter un ≈ìil √† un rapport de
couverture](/post/code-coverage-taux-couverture-tests/).

## Les tests superficiels

> ce rapport de bug est surprenant, on a un test qui v√©rifie exactement ce comportement\
> [quelques minutes plus tard‚Ä¶]\
> aaah reproduit, le test ne v√©rifiait rien en fait üò†

√âcrire des tests c'est bien, √©crire des tests qui v√©rifient r√©ellement un
comportement c'est encore mieux. Par exemple, imaginons que vous vouliez tester
une API HTTP plus ou moins RESTful qui permette de cr√©er une entit√© quelconque
avec une requ√™te `POST` et qui lorsque tout se passe bien renvoie [une r√©ponse
201](https://httpstatuses.com/201). Si les tests de cette fonctionnalit√© se
contentent de ne v√©rifier que le statut HTTP ou quelques trucs dans le contenu
de la r√©ponse, eh bien vous avez un test (tr√®s) superficiel, il ne teste que
quelques d√©tails et passe √† c√¥t√© de la fonctionnalit√© principale qui pourrait
tr√®s bien √™tre compl√®tement cass√©e sans que le test suppos√© la couvrir n'√©choue.
Bon c'est mieux que rien mais √ßa donne un faux sentiment de s√©curit√©.

Pour valider la qualit√© et la pr√©cision des tests, il est possible d'employer
une technique appeler [Mutation
testing](https://blog.ippon.fr/2020/05/20/le-mutation-testing-ou-comment-tester-ses-tests/).
Il s'agit en quelques sorte de tester les tests en introduisant artificiellement
des bugs pour v√©rifier sur les tests √©chouent, si ce n'est pas le cas, vous avez
probablement des tests superficiels¬†!

## Les tests _flaky_

> ptet bien que oui, ptet bien que non
>
> -- un test _flaky_

Le grand classique des tests qui √©chouent seulement de temps √† autre sans raison
apparente. Ce type de probl√®me peut √™tre particuli√®rement compliqu√© √†
diagnostiquer et plus les tests sont de haut niveau plus le nombre de
d√©pendances augmentent et avec elles les risques de _flakyness_.

Parfois, c'est beaucoup plus simple qu'il n'y para√Æt. Pour l'anecdote, il m'est
d√©j√† arriv√© de subir des √©checs al√©atoires de tests en raison d'une mauvaise
utilisation de [`rand()`](https://www.php.net/rand) dans des _fixtures_ qui
n'avaient de toute mani√®re aucune raison d'√™tre al√©atoires.

Malheureusement sur cet aspect, il n'y a pas de solution magique. Je dirais que
le principal est de surveiller activement ce genre de comportement et surtout ne
pas laisser la situation s'enliser car les choses auront plut√¥t tendance √†
empirer qu'√† se r√©soudre d'elles-m√™mes.

## Les tests difficiles √† interpr√©ter car trop verbeux

> Ah je crois que les tests sont en train d'√©chouer‚Ä¶ Ah non en fait, ah je sais
> pas, c'est normal d'avoir 3000 lignes sur la sortie apr√®s une minute
> d'√©x√©cution sur la CI¬†?
>
> -- un¬∑e d√©v√©loppeur¬∑se proche de la noyade

Ce probl√®me est assez typique des tests avec du code en JavaScript, il faut dire
que les m√©thodes `console.log` et consorts sont tellement faciles d'acc√®s,
qu'elles finissent par √™tre utilis√©es avec un peu trop de z√®le au moindre cas
limite. La sortie des tests finit par √™tre un joyeux bazar au point que
parfois il devient difficile de savoir si les tests passent ou √©chouent.
[Jest propose une option
`--silent`](https://jestjs.io/docs/cli#--silent) mais pour moi cela
s'apparente plus √† mettre la poussi√®re sous le tapis qu'√† r√©soudre le soucis.
Dans un monde id√©al, les tests devraient √™tre silencieux et si quelque chose
n'est pas correct, ils devraient juste √©chouer; [oui, prop-types c'est quoi que
je regarde en fron√ßant les sourcils](https://github.com/facebook/prop-types/issues/28)¬†!

## Les tests incompr√©hensibles car r√©dig√©s avec des termes inconnus

> Entretien d'embauche¬†:
>
> - nous on fait du <abbr title="Domain Driven
> Design">DDD</abbr> et on a des tests en mode <abbr title="Behavior Driven
> Development">BDD</abbr>
> - super, vous mettez l'accent sur la qualit√©, o√π est ce que je signe¬†?

En r√©alit√©, c'√©tait l'un des plus gros mensonges de ma
carri√®re professionnelle‚Ä¶ Il y avait vaguement [une approche DDD
technique ou
tactique](https://www.lilobase.me/le-domain-driven-design-sous-langle-strategique-une-introduction/)
et oui des tests √©crits en Gherkin mais [sans l'aspect
comportement ni la plupart du temps le moindre bout de domaine](https://blog.ippon.fr/2021/02/24/4-idees-recues-sur-le-bdd-behavior-driven-development/)
et je parle pas de l'impl√©mentation des phrases‚Ä¶

Bref, qu'on fasse du BDD ou non, en lisant les tests, le ou la d√©veloppeur¬∑se
doit pouvoir comprendre ce que fait le composant/la fonction/l'API test√©e et
comment elle est suppos√©e √™tre utilis√©e. C'est pourquoi le nommage doit √™tre
particuli√®rement soign√©. Il est aussi √©vident que les tests sont des bouts de
code qui vont n√©cessiter de la maintenance. Dans ces conditions appliquer [les
principes de _clean code_](/post/clean-code/) est plus que jamais une bonne id√©e
que votre futur¬∑e vous appr√©ciera sans aucun doute.

## Les tests qui √©chouent avec un message d'erreur cryptique

En contr√¥lant la verbosit√© et en soignant le nommage, on √©vite pas mal d'√©cueils
√† ce niveau. Malgr√© tout, lorsqu'on √©crit des tests, il faut toujours avoir √†
l'esprit que le but est qu'ils √©chouent en communiquant clairement le probl√®me.
Par exemple, si vous utilisez [les assertions de
PHPUnit](https://phpunit.readthedocs.io/en/9.5/assertions.html), il est plus que
probable que vous devriez penser √† utiliser le param√®tre optionnel `$message`
pour am√©liorer cet aspect. Toujours avec PHPUnit, l'utilisation de _data
provider_ permet g√©n√©ralement de tester rapidement un grand nombre combinaisons
mais dans ce cas,
[bien nommer chaque combinaison am√©liorera grandement la compr√©hensibilit√© d'un
√©ventuel
√©chec](https://phpunit.readthedocs.io/en/9.5/writing-tests-for-phpunit.html#writing-tests-for-phpunit-data-providers-examples-datatest1-php)
et en bonus devoir trouver un nom √† chaque combinaison, vous forcera peut-√™tre √†
[simplifier votre API](/post/au-cas-ou/) et/ou √† d√©tecter des cas qui n'ont
aucun sens dans votre domaine.

## Les tests qui _mockent_ l'Univers

Un grand classique d'une strat√©gie de tests pas vraiment r√©fl√©chie du genre
_tout doit √™tre test√© unitairement_ et/ou d'un
peu trop d'attention port√©e [au taux de
couverture](/post/code-coverage-taux-couverture-tests/). En soit, il est normal
de _mocker_ les d√©pendances d'un composant que l'on souhaite tester
unitairement. N√©anmoins, si le composant en question a beaucoup de d√©pendances
et/ou [des d√©pendances qui proviennent d'autres projets](https://matthiasnoback.nl/2018/02/mocking-at-architectural-boundaries-persistence-and-time/),
il est clair que le test risque d'√™tre p√©nible √† maintenir et pire, il pourrait
m√™me √™tre un frein au _refactoring_ et √† l'√©volution du composant test√©¬†! Pour
√©viter cela, deux solutions compl√©mentaires¬†:

1. D'un c√¥t√©, on peut consid√©rer que le composant en question n√©cessite un
   _refactoring_ pour diminuer le nombre de d√©pendances (avec autant de
   d√©pendances il est probable qu'il ait un peu trop de responsabilit√©s) et pour
   s'abstraire des d√©pendances que l'on ne contr√¥le pas, l'id√©e √©tant de tendre
   vers [une architecture hexagonale ou en
   oignon](https://fr.wikipedia.org/wiki/Architecture_hexagonale_(logiciel)#Variantes).
1. On peut aussi opter pour des tests de plus haut niveau comme des tests
   d'int√©gration ou fonctionnels pour √©viter de tout _mocker_ et tester un peu
   moins unitairement chaque composant. Le prix √† payer est une mise en place
   des tests potentiellement un peu plus complexe et √©ventuellement un peu
   moins de couverture (certains cas limites peuvent √™tre plus difficiles √†
   obtenir) mais en contrepartie ces tests seront plus simples et surtout √†
   partir du moment o√π la fonctionnalit√© est couverte, on peut la retravailler
   en tout s√©r√©nit√© et √©ventuellement se lancer dans le _refactoring_ √©voqu√©
   pr√©c√©demment.

Comme toujours en d√©veloppement, c'est une affaire de contexte, de compromis et
de strat√©gie.

## Les tests interd√©pendants

> Je comprends pas, j'ai chang√© 3 d√©tails dans une m√©thode et j'ai 30 tests qui
> √©chouent dans les tests fonctionnels de l'API REST
>
> -- Un¬∑e d√©veloppeur¬∑se devant un ch√¢teau de cartes qui s'effondre

Alors bien s√ªr il est possible de faire √©chouer beaucoup de tests avec peu de
changements mais la derni√®re fois o√π je me suis trouv√© en face de cette
situation la raison √©tait un peu diff√©rente. J'avais effectivement introduit un bug
dans la cr√©ation d'une entit√© quelconque ce qui aurait du faire √©chouer 3
tests mais en fait les 27 autres assumaient que ces premiers tests
passaient pour utiliser les donn√©es cr√©√©es‚Ä¶ une belle mani√®re de rendre les
choses confuses. J'ai aussi vu des suites de tests ou certains tests en
appellent directement d'autres voire m√™me issues d'une autre suite de tests¬†! L√†
encore, c'est l'effet ch√¢teau de cartes au moindre bug.

---

Tout ceci est du v√©cu; heureusement pour ma sant√©
mentale, j'ai jamais vu de projet qui cumulait tous ces _antipatterns_, apr√®s
j'ai aussi vu pas mal de projets _critiques_ sans aucun test üòÄ

Plus s√©rieusement, tous ces d√©fauts ne sont pas seulement aga√ßants, ce sont
surtout des obstacles [pour travailler
efficacement](/post/maximiser-efficacite-developpeurs/) et pour produire un
logiciel robuste et de qualit√© ce qui est quand m√™me un comble pour une
technique justement suppos√©e am√©liorer ces aspects.
