---
title: "TestsÂ : mon top 8 des anti-patrons les plus agaÃ§ants"
tags: bonnes pratiques, unit test, qualitÃ©, code, behaviour driven development, tdd, php, javascript, ingÃ©nierie logicielle
lang: fr
published: 2021-04-08
photos:
    - images/angry-cat.jpg
---

Il y a un peu plus trois ans, je publiais [Au fait, c'est quoi un bon test
unitaire, d'intÃ©gration ou
fonctionnelÂ ?](/post/bon-test-unitaire-integration-fonctionnel), un billet oÃ¹ je
dÃ©crivais quelques caractÃ©ristiques d'une bonne suite de tests logiciel. Ce
billet reste d'actualitÃ© mais depuis, je me suis frottÃ© Ã  plusieurs
environnements accumulant parfois plusieurs _antipatterns_, certains
particuliÃ¨rement pÃ©nibles. Alors cette fois ci, je prends le sujet dans l'autre
sens, voici une sÃ©lection des anti-patrons liÃ©s aux tests que j'ai pu croisÃ©s
plus ou moins rÃ©cemment.


<figure class="object-center bordered">
    <img loading="lazy" src="/images/660x/angry-cat.jpg" alt="Un chat donnant l'impression d'Ãªtre agacÃ©">
    <footer>
    Photo par <a
    href="https://pixabay.com/users/skitterphoto-324082/">Skitterphoto</a>
    </footer>
</figure>

## Les tests pas lancÃ©s automatiquement

Une suite de tests qui n'est pas lancÃ©e automatiquement ne sert (quasiment) Ã 
rien. Bien sÃ»r, si on se lance dans un effort pour Ã©crire des tests (en
particulier des tests _end to end_ ou fonctionnels), cet Ã©tat peut Ãªtre
_transitoire_ le temps d'investir suffisamment de temps pour automatiser
l'ensemble mais clairement il faut essayer d'en sortir le plus rapidement
possible sinon arrivera invariablement le moment oÃ¹ unÂ·e dÃ©veloppeurÂ·se cassera
quelque chose sans s'en rendre compte et gÃ©nÃ©ralement, il faut pas longtemps ğŸ˜‰

Dans cette catÃ©gorie, il arrive aussi qu'une partie des tests ne soient pas
lancer automatiquement, par exemple lors de l'ajout de code dans un endroit pour
lequel le _test runner_ n'est pas configurÃ©. C'est assez vicieux et pas
forcÃ©ment Ã©vident Ã  dÃ©tecter; on peut Ã©ventuellement
surveiller l'Ã©volution [du taux de couverture et/ou jeter un Å“il Ã  un rapport de
couverture](/post/code-coverage-taux-couverture-tests/).

## Les tests superficiels

> ce rapport de bug est surprenant, on a un test qui vÃ©rifie exactement ce comportement\
> [quelques minutes plus tardâ€¦]\
> aaah reproduit, le test ne vÃ©rifiait rien en fait ğŸ˜ 

Ã‰crire des tests c'est bien, Ã©crire des tests qui vÃ©rifient rÃ©ellement un
comportement c'est encore mieux. Par exemple, imaginons que vous vouliez tester
une API HTTP plus ou moins RESTful qui permette de crÃ©er une entitÃ© quelconque
avec une requÃªte `POST` et qui lorsque tout se passe bien renvoie [une rÃ©ponse
201](https://httpstatuses.com/201). Si les tests de cette fonctionnalitÃ© se
contentent de ne vÃ©rifier que le statut HTTP ou quelques trucs dans le contenu
de la rÃ©ponse, eh bien vous avez un test (trÃ¨s) superficiel, il ne teste que
quelques dÃ©tails et passe Ã  cÃ´tÃ© de la fonctionnalitÃ© principale qui pourrait
trÃ¨s bien Ãªtre complÃ¨tement cassÃ©e sans que le test supposÃ© la couvrir n'Ã©choue.
Bon c'est mieux que rien mais Ã§a donne un faux sentiment de sÃ©curitÃ©.

Pour valider la qualitÃ© et la prÃ©cision des tests, il est possible d'employer
une technique appeler [Mutation
testing](https://blog.ippon.fr/2020/05/20/le-mutation-testing-ou-comment-tester-ses-tests/).
Il s'agit en quelques sorte de tester les tests en introduisant artificiellement
des bugs pour vÃ©rifier sur les tests Ã©chouent, si ce n'est pas le cas, vous avez
probablement des tests superficielsÂ !

## Les tests _flaky_

> ptet bien que oui, ptet bien que non
>
> -- un test _flaky_

Le grand classique des tests qui Ã©chouent seulement de temps Ã  autre sans raison
apparente. Ce type de problÃ¨me peut Ãªtre particuliÃ¨rement compliquÃ© Ã 
diagnostiquer et plus les tests sont de haut niveau plus le nombre de
dÃ©pendances augmentent et avec elles les risques de _flakyness_.

Parfois, c'est beaucoup plus simple qu'il n'y paraÃ®t. Pour l'anecdote, il m'est
dÃ©jÃ  arrivÃ© de subir des Ã©checs alÃ©atoires de tests en raison d'une mauvaise
utilisation de [`rand()`](https://www.php.net/rand) dans des _fixtures_ qui
n'avaient de toute maniÃ¨re aucune raison d'Ãªtre alÃ©atoires.

Malheureusement sur cet aspect, il n'y a pas de solution magique. Je dirais que
le principal est de surveiller activement ce genre de comportement et surtout ne
pas laisser la situation s'enliser car les choses auront plutÃ´t tendance Ã 
empirer qu'Ã  se rÃ©soudre d'elles-mÃªmes.

## Les tests difficiles Ã  interprÃ©ter car trop verbeux

> Ah je crois que les tests sont en train d'Ã©chouerâ€¦ Ah non en fait, ah je sais
> pas, c'est normal d'avoir 3000 lignes sur la sortie aprÃ¨s une minute
> d'Ã©xÃ©cution sur la CIÂ ?
>
> -- unÂ·e dÃ©vÃ©loppeurÂ·se proche de la noyade

Ce problÃ¨me est assez typique des tests avec du code en JavaScript, il faut dire
que les mÃ©thodes `console.log` et consorts sont tellement faciles d'accÃ¨s,
qu'elles finissent par Ãªtre utilisÃ©es avec un peu trop de zÃ¨le au moindre cas
limite. La sortie des tests finit par Ãªtre un joyeux bazar au point que
parfois il devient difficile de savoir si les tests passent ou Ã©chouent.
[Jest propose une option
`--silent`](https://jestjs.io/docs/cli#--silent) mais pour moi cela
s'apparente plus Ã  mettre la poussiÃ¨re sous le tapis qu'Ã  rÃ©soudre le soucis.
Dans un monde idÃ©al, les tests devraient Ãªtre silencieux et si quelque chose
n'est pas correct, ils devraient juste Ã©chouer; [oui, prop-types c'est quoi que
je regarde en fronÃ§ant les sourcils](https://github.com/facebook/prop-types/issues/28)Â !

## Les tests incomprÃ©hensibles car rÃ©digÃ©s avec des termes inconnus

> Entretien d'embaucheÂ :
>
> - nous on fait du <abbr title="Domain Driven
> Design">DDD</abbr> et on a des tests en mode <abbr title="Behavior Driven
> Development">BDD</abbr>
> - super, vous mettez l'accent sur la qualitÃ©, oÃ¹ est ce que je signeÂ ?

En rÃ©alitÃ©, c'Ã©tait l'un des plus gros mensonges de ma
carriÃ¨re professionnelleâ€¦ Il y avait vaguement [une approche DDD
technique ou
tactique](https://www.lilobase.me/le-domain-driven-design-sous-langle-strategique-une-introduction/)
et oui des tests Ã©crits en Gherkin mais [sans l'aspect
comportement ni la plupart du temps le moindre bout de domaine](https://blog.ippon.fr/2021/02/24/4-idees-recues-sur-le-bdd-behavior-driven-development/)
et je parle pas de l'implÃ©mentation des phrasesâ€¦

Bref, qu'on fasse du BDD ou non, en lisant les tests, le ou la dÃ©veloppeurÂ·se
doit pouvoir comprendre ce que fait le composant/la fonction/l'API testÃ©e et
comment elle est supposÃ©e Ãªtre utilisÃ©e. C'est pourquoi le nommage doit Ãªtre
particuliÃ¨rement soignÃ©. Il est aussi Ã©vident que les tests sont des bouts de
code qui vont nÃ©cessiter de la maintenance. Dans ces conditions appliquer [les
principes de _clean code_](/post/clean-code/) est plus que jamais une bonne idÃ©e
que votre futurÂ·e vous apprÃ©ciera sans aucun doute.

## Les tests qui Ã©chouent avec un message d'erreur cryptique

En contrÃ´lant la verbositÃ© et en soignant le nommage, on Ã©vite pas mal d'Ã©cueils
Ã  ce niveau. MalgrÃ© tout, lorsqu'on Ã©crit des tests, il faut toujours avoir Ã 
l'esprit que le but est qu'ils Ã©chouent en communiquant clairement le problÃ¨me.
Par exemple, si vous utilisez [les assertions de
PHPUnit](https://phpunit.readthedocs.io/en/9.5/assertions.html), il est plus que
probable que vous devriez penser Ã  utiliser le paramÃ¨tre optionnel `$message`
pour amÃ©liorer cet aspect. Toujours avec PHPUnit, l'utilisation de _data
provider_ permet gÃ©nÃ©ralement de tester rapidement un grand nombre combinaisons
mais dans ce cas,
[bien nommer chaque combinaison amÃ©liorera grandement la comprÃ©hensibilitÃ© d'un
Ã©ventuel
Ã©chec](https://phpunit.readthedocs.io/en/9.5/writing-tests-for-phpunit.html#writing-tests-for-phpunit-data-providers-examples-datatest1-php)
et en bonus devoir trouver un nom Ã  chaque combinaison, vous forcera peut-Ãªtre Ã 
[simplifier votre API](/post/au-cas-ou/) et/ou Ã  dÃ©tecter des cas qui n'ont
aucun sens dans votre domaine.

## Les tests qui _mockent_ l'Univers

Un grand classique d'une stratÃ©gie de tests pas vraiment rÃ©flÃ©chie du genre
_tout doit Ãªtre testÃ© unitairement_ et/ou d'un
peu trop d'attention portÃ©e [au taux de
couverture](/post/code-coverage-taux-couverture-tests/). En soit, il est normal
de _mocker_ les dÃ©pendances d'un composant que l'on souhaite tester
unitairement. NÃ©anmoins, si le composant en question a beaucoup de dÃ©pendances
et/ou [des dÃ©pendances qui proviennent d'autres projets](https://matthiasnoback.nl/2018/02/mocking-at-architectural-boundaries-persistence-and-time/),
il est clair que le test risque d'Ãªtre pÃ©nible Ã  maintenir et pire, il pourrait
mÃªme Ãªtre un frein au _refactoring_ et Ã  l'Ã©volution du composant testÃ©Â ! Pour
Ã©viter cela, deux solutions complÃ©mentairesÂ :

1. D'un cÃ´tÃ©, on peut considÃ©rer que le composant en question nÃ©cessite un
   _refactoring_ pour diminuer le nombre de dÃ©pendances (avec autant de
   dÃ©pendances il est probable qu'il ait un peu trop de responsabilitÃ©s) et pour
   s'abstraire des dÃ©pendances que l'on ne contrÃ´le pas, l'idÃ©e Ã©tant de tendre
   vers [une architecture hexagonale ou en
   oignon](https://fr.wikipedia.org/wiki/Architecture_hexagonale_(logiciel)#Variantes).
1. On peut aussi opter pour des tests de plus haut niveau comme des tests
   d'intÃ©gration ou fonctionnels pour Ã©viter de tout _mocker_ et tester un peu
   moins unitairement chaque composant. Le prix Ã  payer est une mise en place
   des tests potentiellement un peu plus complexe et Ã©ventuellement un peu
   moins de couverture (certains cas limites peuvent Ãªtre plus difficiles Ã 
   obtenir) mais en contrepartie ces tests seront plus simples et surtout Ã 
   partir du moment oÃ¹ la fonctionnalitÃ© est couverte, on peut la retravailler
   en tout sÃ©rÃ©nitÃ© et Ã©ventuellement se lancer dans le _refactoring_ Ã©voquÃ©
   prÃ©cÃ©demment.

Comme toujours en dÃ©veloppement, c'est une affaire de contexte, de compromis et
de stratÃ©gie.

## Les tests interdÃ©pendants

> Je comprends pas, j'ai changÃ© 3 dÃ©tails dans une mÃ©thode et j'ai 30 tests qui
> Ã©chouent dans les tests fonctionnels de l'API REST
>
> -- UnÂ·e dÃ©veloppeurÂ·se devant un chÃ¢teau de cartes qui s'effondre

Alors bien sÃ»r il est possible de faire Ã©chouer beaucoup de tests avec peu de
changements mais la derniÃ¨re fois oÃ¹ je me suis trouvÃ© en face de cette
situation la raison Ã©tait un peu diffÃ©rente. J'avais effectivement introduit un bug
dans la crÃ©ation d'une entitÃ© quelconque ce qui aurait du faire Ã©chouer 3
tests mais en fait les 27 autres assumaient que ces premiers tests
passaient pour utiliser les donnÃ©es crÃ©Ã©esâ€¦ une belle maniÃ¨re de rendre les
choses confuses. J'ai aussi vu des suites de tests ou certains tests en
appellent directement d'autres voire mÃªme issues d'une autre suite de testsÂ ! LÃ 
encore, c'est l'effet chÃ¢teau de cartes au moindre bug.

---

Tout ceci est du vÃ©cu; heureusement pour ma santÃ©
mentale, j'ai jamais vu de projet qui cumulait tous ces _antipatterns_, aprÃ¨s
j'ai aussi vu pas mal de projets _critiques_ sans aucun test ğŸ˜€

Plus sÃ©rieusement, tous ces dÃ©fauts ne sont pas seulement agaÃ§ants, ce sont
surtout des obstacles [pour travailler
efficacement](/post/maximiser-efficacite-developpeurs/) et pour produire un
logiciel robuste et de qualitÃ© ce qui est quand mÃªme un comble pour une
technique justement supposÃ©e amÃ©liorer ces aspects.
